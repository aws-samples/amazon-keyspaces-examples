{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73a0913e-c6df-436c-bfd6-8d9a98f8d8ba",
   "metadata": {},
   "source": [
    "# Keyspaces and Langchain chat history\n",
    "--- \n",
    "\n",
    "LangChain is a popular framework for developing applications powered by large language models (LLMs). LangChain provides tools and abstractions to improve the customization, accuracy, and relevancy of the information the models generate. For example, developers may want to build chat applications and preserve chat history or break down complex prompts into several steps or allow LLMs to access transactional data in realtime without without retraining. \n",
    "\n",
    "Amazon Keyspaces (for Apache Cassandra) is a scalable, highly available, and serverless database service on AWS. With Amazon Keyspaces you can build applications that can scale to thousands of requests per second with consistent latencies. With Amazon Keyspaces, you can use Apache Cassandra application code and developer tools that you use today.  Amazon Keyspaces is serverless, so you pay for only the resources you use and the service can automatically scale tables up and down in response to application traffic. \n",
    "\n",
    "This notebook provides a quickstart for using Amazon Keyspaces with Langchain and Amazon Bedrock. In this notebook you wll:\n",
    " - Connect to Keyspaces from Langchain\n",
    " - Leverage Chat model in Amazon bedrock\n",
    " - Store chat history in Amazon Keyspaces\n",
    "\n",
    "\n",
    "Chat models are a type of language model that operates on a conversational level, taking in chat messages as inputs and generating chat messages as outputs, rather than dealing with plain text. LangChain provides a standardized interface for developers to interact with various chat model providers, such as AWS, OpenAI, Cohere, and Hugging Face, allowing for seamless integration and utilization of these models across different platforms.\n",
    "\n",
    "One popular application of chat models is task decomposition, a technique that aims to break down complex tasks into smaller, more manageable subtasks. Instead of presenting the model with a large, monolithic input, task decomposition helps agents or models handle difficult tasks by dividing them into more digestible steps. This can be achieved through methods like Chain of Thought (CoT) or Tree of Thoughts, which guide the model through a step-by-step thought process or explore multiple reasoning paths at each stage.\n",
    "\n",
    "Task decomposition can be implemented in various ways, such as using simple prompting with a language model (LLM), providing task-specific instructions, or leveraging human inputs. For instance, an LLM can be prompted with phrases like \"Steps for XYZ\" to encourage it to break down a task into steps, or it can be given specific instructions like \"Write a story outline\" to initiate the task decomposition process. Additionally, human inputs can be incorporated to decompose tasks into smaller, more manageable steps, allowing for a collaborative approach.\n",
    "\n",
    "To enhance the effectiveness of task decomposition, developers often combine chat models with NoSQL storage solutions. NoSQL databases, with their flexible schema and scalability, provide an ideal storage solution for managing the varying structures and potentially large volumes of subtasks generated during the decomposition process. By integrating chat models and NoSQL storage, developers can create powerful task management and chat applications that leverage the capabilities of generative AI while ensuring efficient storage and retrieval of task-related data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f356ec8-f88a-412c-b17b-dd31b9f06ac7",
   "metadata": {},
   "source": [
    "---\n",
    "## Prequisits\n",
    "Checkout the following notebook to get familiar with Bedrock and Sagemaker studio Jupyter notebooks. \n",
    "[Amazon Bedrock prerequisites](https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/00_Prerequisites/bedrock_basics.ipynb) \n",
    "\n",
    "### IAM\n",
    "If you are running this notebook from Amazon Sagemaker Studio and your Sagemaker Studio execution role has permissions to access Bedrock and Amazon Keyspaces to run the cells below as-is. This is also the case if you are running these notebooks from a computer whose default AWS credentials have access to Bedrock. \n",
    "\n",
    "### Python libraries\n",
    "Install python dependencies including:\n",
    "* boto3, langchain, cassandra driver, and Keyspaces Sigv4 Authentication\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bf7064-b83b-42e2-8dd3-a3c8cb9bcf6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\" \\\n",
    "    \"cassio>=0.1.0\" \\\n",
    "    \"cassandra-sigv4>=4.0.2\"\n",
    "\n",
    "%pip install --upgrade --quiet langchain langchain-community langchainhub langchain-anthropic bs4  langchain-aws\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1e8d47-9996-4fb7-9fed-2c8945952dd2",
   "metadata": {},
   "source": [
    "--- \n",
    "### Connect and authenticate using IAM\n",
    "\n",
    "The following cells will test authentication with bedrock and Amazon Keyspaces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02f6abb-4855-4f4f-8a89-55b8fd843a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "print(\"test auth\") \n",
    "\n",
    "boto3_bedrock = boto3.client('bedrock')\n",
    "\n",
    "boto3_bedrock.list_foundation_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e926216-4794-4a0f-aed4-4e34d9b66396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster, ExecutionProfile, EXEC_PROFILE_DEFAULT\n",
    "from ssl import SSLContext, PROTOCOL_TLSv1_2 , CERT_REQUIRED\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "import boto3\n",
    "from cassandra_sigv4.auth import SigV4AuthProvider\n",
    "from cassandra import ConsistencyLevel\n",
    "\n",
    "ssl_context = SSLContext(PROTOCOL_TLSv1_2)\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "\n",
    "auth_provider = SigV4AuthProvider(boto_session)\n",
    "\n",
    "profile = ExecutionProfile(\n",
    "    consistency_level=ConsistencyLevel.LOCAL_QUORUM\n",
    ")\n",
    "\n",
    "cluster = Cluster(['cassandra.us-east-1.amazonaws.com'], ssl_context=ssl_context, auth_provider=auth_provider,\n",
    "                  port=9142, execution_profiles={EXEC_PROFILE_DEFAULT: profile})\n",
    "\n",
    "session = cluster.connect()\n",
    "\n",
    "r = session.execute('select * from system_schema.keyspaces')\n",
    "\n",
    "print(r.current_rows[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b243cd-07bf-4944-804d-5e9b46e9da5e",
   "metadata": {},
   "source": [
    "---\n",
    "### Create a new Keyspace\n",
    "\n",
    "A Keyspace is a logical collection of tables. For this notebook we will create a new Keyspace which will hold a few different tables in this example.  In Amazon Keyspaces, resources are created asychronously. When we create a new Keyspace we wait for a few seconds, and then check if the Keyspace is avialable. If the Keyspace is not created it will not return any rows. At this point we are purely using the Cassandra python driver. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eae6ac3d-6182-4d1a-8040-0c68d27f7f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(keyspace_name='keyspaces_genaiy', durable_writes=True, replication=OrderedMapSerializedKey([('class', 'org.apache.cassandra.locator.SimpleStrategy'), ('replication_factor', '3')]))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "session.execute(\"CREATE KEYSPACE if not exists keyspaces_genai WITH REPLICATION = {'class':'SingleRegionStrategy'}\")\n",
    "\n",
    "\n",
    "query = \"SELECT * FROM system_schema_mcs.keyspaces WHERE keyspace_name = 'keyspaces_genai'\"\n",
    "\n",
    "for x in range(5):\n",
    "    result = session.execute(query)\n",
    "    if result:\n",
    "        print(result.current_rows)\n",
    "        break\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceedcbe2-7976-444d-9418-027e4a9d48da",
   "metadata": {},
   "source": [
    "--- \n",
    "## Lanchain chat history\n",
    "\n",
    "In many Q&A applications we want to allow the user to have a back-and-forth conversation with GenAI, meaning the application maintains “memory” of past questions and answers, and some logic for incorporating those into its current thinking.\n",
    "\n",
    "To use Keyspaces as store for langchain chat history you will need to pass in the session and specify the Keyspace for the message history. This will create a new table in the Keyspace called \"message store\".  The message store is a simple model consisting of a partition_id and a row_id as a primary key and body_blob text to store raw message. The partition_id allows us to seperate history for each user session so we can scale out the number of user sessions horizontally in Amazon Keyspaces.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8a4f8f-6312-4c56-9873-dbd1dda71bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import (\n",
    "    CassandraChatMessageHistory,\n",
    ")\n",
    "\n",
    "message_history = CassandraChatMessageHistory(\n",
    "    session_id=\"test-user-session\",\n",
    "    session=session,\n",
    "    keyspace='aws',\n",
    ")\n",
    "\n",
    "## Similar to the \"keyspaces_genai\" keyspace we created earlier, table resources are also created Asynchronously. \n",
    "## After creating the CassandraChatMessageHistory, \n",
    "## you can make sure the new \"message_store\" table is active before messages can be stored. \n",
    "## In the following example you will inspect the status field for 'ACTIVE'.\n",
    "query = \"select status, default_time_to_live, custom_properties from system_schema_mcs.tables where keyspace_name = 'keyspaces_genai' and table_name = 'message_store'\"\n",
    "\n",
    "for x in range(5):\n",
    "    result = session.execute(query)\n",
    "    if any(row[0] == 'ACTIVE' for row in result):\n",
    "        print(result.current_rows)\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d1b18f-3b6c-4810-8d08-4a22682abfea",
   "metadata": {},
   "source": [
    "---\n",
    "### Test CassandraChatMessageHistory functionality\n",
    "\n",
    "Now that the message_store tabel is complete you can use the CRUD operations to add and retrieve message history. Later we will integrate it directly with Chat Model without having to manually add and retrieve messagges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306931e0-3952-4c0c-b755-732b947cc17a",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "message_history.add_user_message(\"What age should I stop playing basketball?\")\n",
    "\n",
    "message_history.add_ai_message(\"The answer to the ultimate question of life, the universe, and everything is 42\")\n",
    "\n",
    "print(await message_history.aget_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47f741-6055-4031-9581-2bd5455e7da2",
   "metadata": {},
   "source": [
    "---\n",
    "### Connect to ChatBedrcok\n",
    "\n",
    "ChatBedrock is a [Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI. Using Amazon Bedrock, you can easily experiment with and evaluate top FMs for your use case, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources. Since Amazon Bedrock is serverless, you don’t have to manage any infrastructure, and you can securely integrate and deploy generative AI capabilities into your applications using the AWS services you are already familiar with.\n",
    "\n",
    "In the following notebook cell you will Creat a Chatmodel that uses Amazon Bedrock and Anthropic claude-3.  You will tranlate a set of text from English to French and stream the result. Streaming allows you to return response in chunks without waiting for the full generated response. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bca523ae-4c2e-4afb-92c0-cb531e4e340b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voici la traduction en français :\n",
      "\n",
      "\"Bonjour le monde !\""
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    model_kwargs={\"temperature\": 0.1},\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"Translate this sentence from English to French. \\\"Hellow World!\\\"\"\n",
    "    )\n",
    "]\n",
    "llm.invoke(messages)\n",
    "\n",
    "for chunk in llm.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095a46e5-21d1-4a7d-a9e8-23b448809ffd",
   "metadata": {},
   "source": [
    "### Test the LLM \"memory\"\n",
    "\n",
    "Now we will test the llm's ability to recall the previous request. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f09a6ec-9f87-4e66-842b-c33fb73f620b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm afraid I don't have any specific context about a previous translation request from you. I don't have a long-term memory of our prior conversations. Could you please restate what you need translated?"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"Do you recall what I asked you to translate?\"\n",
    "    )\n",
    "]\n",
    "llm.invoke(messages)\n",
    "for chunk in llm.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91baf105-4da7-4367-ab31-96143664de9d",
   "metadata": {},
   "source": [
    "---\n",
    "### Integrating Chathistory with Chatmodel\n",
    "\n",
    "Previosuly you were manually updating and retrieving chat history. In practice, you could insert it into the each input.  In a real Q&A application we’ll want some way of persisting chat history and some way of automatically inserting and updating it. You may want to be able to do this so you could switch out the unerlying LLM model for another, or switch out the chat history store for another. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e00d4d-63ca-4c88-9a91-46ea500ea01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: CassandraChatMessageHistory(\n",
    "    session_id=session_id,\n",
    "    session=session,\n",
    "    keyspace='keyspaces_genai',\n",
    "),\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"mike-session\"}}\n",
    "\n",
    "chain_with_history.invoke({\"question\": \"Hi! I'm Mike\"}, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605d5954-6189-4782-b70a-d9bf68e51ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_history.invoke({\"question\": \"Hey, do you recall my name?\"}, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d336bd3-e5dd-414b-9910-d7bf2fc8672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_history.invoke({\"question\": \"How do your remember my name? Do you store it in Amazon Keyspaces?\"}, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50e0707-4c6e-4854-a34e-7a2f5e0b3b4c",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "As you can see the LLM does not know you are storing the history in Amazon Keyspaces, implmented CassandraChatMessageHistory, or even using Langchain. The CassandraChatMessageHistory is passed to input pf the underlying LLM.  Using Amazon Keyspaces will allow you to store chat history with single digit ms latencies, and scale as the number of users grow. In other examples we will show how Keyspaces can be used as Document Loader and LLM Cache. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af84b96-209f-439c-aac8-e8b9d7a88261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
