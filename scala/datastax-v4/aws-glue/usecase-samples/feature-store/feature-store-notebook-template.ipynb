{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# Amazon Keyspaces feature store demo\n\nOne of the central elements of an ML platform is the online feature store. It allows multiple ML models to retrieve hundreds or thousands of features with low latency, and enables application of AI for real-time use cases. Customers such as Sumup created feature stores on Amazon Keyspaces which processes thousands of transactions every second, with volume spikes during peak hours, and has steady growth that doubles the number of transactions every year. Because of this, the ML platform requires its low-latency feature store to be also highly reliable and scalable.\n\nTo train ML models, we need historical data. During this phase, data scientists experiment with different features to test which ones produce the best model. From a platform perspective, we need to support bulk read and write operations. Read latency isnâ€™t critical at this stage because the data is read into training jobs. After the models are trained and moved to production for real-time inference, we have the following requirements for the platform change: we need to support low-latency reads and use only the latest features data.This interactive notebook steps reproduces some of the steps to integrate Amazon Keyspaces as an online feature store to power ML/AI workloads. \n\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### First: Intro into Notebooks and the Glue Interactive session. \n\nTo start using your notebook you need to start an AWS Glue Interactive Session. Jupyter Magics are commands that can be run at the beginning of a cell or as a whole cell body. Magics start with % for line-magics and %% for cell-magics. Line-magics such as %region and %connections can be run with multiple magics in a cell, or with code included in the cell body. \n* To run each step click on the cell and hit 'run' button up above\n* To stop an execution click on the stop/iterupt kernal button\n* To restart glue session, execute the magics %stop_session below.\n* If this is your first time proceed to the next step. \n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "#%help\n# The following magic 'stop_session' will stop the glue interactive session to allow you to repeat steps if nessesary.\n# You will need to uncomment the statement and hit run button\n%stop_session",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "### Configure Glue for interactive session\n\nAnother important consideration is that we write a single feature job to populate both feature stores. Otherwise, SumUp would have to maintain two sets of code or pipelines for each feature creation job. We use AWS Glue but you could also use Amazon EMR to create the features using PySpark DataFrames. The same DataFrame is written to both Delta Lake and Amazon Keyspaces, which eliminates the hurdle of having separate pipelines.\n\n![image](https://d2908q01vomqb2.cloudfront.net/b6692ea5df920cad691c20319a6fffd7a4a766b8/2022/07/06/BDB-1587-image001.png)\n\n\nThe following cell will configure glue, pyspark, spark cassandra connector, and delta lake on s3. Execute this step to set the appropriate variables for spark conf that will connect to Amazon Keysapces for storing features later in this example. \n",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%glue_version 3.0\n%%configure \n{\n    \"--max_concurrent_runs\":\"1\",\n    \"--enable-job-insights\":\"true\",\n    \"--enable-metrics\":\"true\",\n    \"--enable-spark-ui\": \"true\",\n    \"--spark-event-logs-path\": \"s3://800S3BUCKET/spark-logs/\",\n    \"--datalake-formats\": \"delta\",\n    \"--enable-observability-metrics\":\"true\",\n    \"--user-jars-first\": \"true\",\n    \"--extra-jars\":\"s3://800S3BUCKET/jars/spark-cassandra-connector-assembly_2.12-3.1.0.jar,s3://800S3BUCKET/jars/aws-sigv4-auth-cassandra-java-driver-plugin-4.0.9-shaded.jar,s3://800S3BUCKET/jars/spark-extension_2.12-2.8.0-3.4.jar,s3://800S3BUCKET/jars/amazon-keyspaces-helpers-1.0-SNAPSHOT.jar\",\n    \"--extra-files\":\"s3://800S3BUCKET/conf/keyspaces-application.conf\",\n    \"--TempDir\": \"s3://800S3BUCKET/temp\",\n     \"--conf\": \"spark.sql.extensions=com.datastax.spark.connector.CassandraSparkExtensions,io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.myCatalog=com.datastax.spark.connector.datasource.CassandraCatalog --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog --conf spark.delta.logStore.class=org.apache.spark.sql.delta.storage.S3SingleDriverLogStore --conf spark.cassandra.connection.config.profile.path=keyspaces-application.conf --conf spark.task.maxFailures=100 --conf spark.cassandra.output.ignoreNulls=true --conf spark.cassandra.output.ignoreNulls=true --conf directJoinSetting=on --conf spark.cassandra.output.concurrent.writes=1 --conf spark.cassandra.output.batch.grouping.key=none --conf spark.cassandra.output.batch.size.rows=10 --conf directJoinSetting=on\"\n}\n",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Connect and show Keyspaces\n\nIn the following cell you will create a new glue interactive session. The session will connect to Amazon Keyspaces through the spark-cassandra-connector and display the current keyspaces. In the previous statement we instantiated a spark sql catalog for Keyspaces named myCatalog. Spark SQL uses an ANSI compliant dialect instead of being Hive compliant. The connector will automatically pushdown all valid predicates to Keyspaces. The Datasource will also automatically project select columns from Keyspaces which are specified in query. \n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql.functions import rand \n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\n\nprint(\"Connected\")\n\n\nspark.sql(\"SHOW NAMESPACES FROM myCatalog\").show()\n\n\n",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Extract dataset\n\nDelta Lake is an open-source storage layer that supports ACID transactions and is fully compatible with Apache Spark, making it highly performant at bulk read and write operations. You can store Delta Lake tables on Amazon Simple Storage Service (Amazon S3), which makes it a good fit for the offline feature store. Data scientists can use this stack to train models against the offline feature store (Delta Lake). When the trained models are moved to production, we switch to using the online feature store (Amazon Keyspaces), which offers the latest features set, scalable reads, and much lower latency.\n\nIn the below example we will first need to read in a sample dataset. You will load the UK energy data into a spark data frame. Update the value of s3_uri to point to the bucket where your data resides. We will also cache the data so spark keeps the dataset in memory while we process it. Finally, the day_date column in the dataset is loaded as a string so we we will create a new column \"day_date\" of type date so we can use it to calulate features over different time windows. This will help develop additional features to expermiment with. Once executed you will see a new schema with date type.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "import pyspark.sql.functions as F\n\ns3_uri = \"s3://800S3BUCKET/sample-data/daily_dataset.csv\"\n\nprint(\"Loading Data\")\n\ndf = (\n    spark\n    .read.format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\")\n    .option(\"inferschema\", \"true\")\n    .load(s3_uri)\n    )\n\ndf.cache()\n\nprint(\"Records Read: {0:,}\".format( df.count() ))\n\n## modify date column\ndf = df.withColumn('day_date', F.to_date('day', 'yyyy-MM-dd'))\n\ndf.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "### Create Features \n\nSelecting relevant features and also engineering new features is key to building high performing models. You will perform count, sum, average and std over different time windows of 30, 60 and 90 days.\n\n",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import Window\nfrom pyspark.sql.functions import when, max, sum, avg, stddev \nfrom datetime import date\nfrom dateutil.relativedelta import relativedelta\nfrom pyspark.sql.functions import col, lit\n\nwindow = Window.partitionBy(\"household_id\").orderBy(F.col('day_date').desc())\n\ndf = df.withColumn(\"energy_sum_3months\", sum(when(df.day_date \\\n                                                             >= (date(2014,2,28) - relativedelta(months=3))\\\n                                                        , col(\"energy_sum\")).otherwise(0))\\\n                                                  .over(window))\n\ndf = df.withColumn(\"energy_sum_6months\", sum(when(df.day_date \\\n                                                             >= (date(2014,2,28) - relativedelta(months=6))\\\n                                                        , col(\"energy_sum\")).otherwise(0))\\\n                                                  .over(window))\n\ndf = df.withColumn(\"energy_sum_1yr\", sum(when(df.day_date \\\n                                                             >= (date(2014,2,28) - relativedelta(months=12))\\\n                                                        , col(\"energy_sum\")).otherwise(0))\\\n                                                  .over(window))\n#------\n# Count\n#------\n\ndf = df.withColumn(\"energy_count_3months\", sum(when(df.day_date \\\n                                                             >= (date(2014,2,28) - relativedelta(months=3))\\\n                                                        , col(\"energy_count\")).otherwise(0))\\\n                                                  .over(window))\n\ndf = df.withColumn(\"energy_count_6months\", sum(when(df.day_date \\\n                                                             >= (date(2014,2,28) - relativedelta(months=6))\\\n                                                        , col(\"energy_count\")).otherwise(0))\\\n                                                  .over(window))\n\ndf = df.withColumn(\"energy_count_1yr\", sum(when(df.day_date \\\n                                                             >= (date(2014,2,28) - relativedelta(months=12))\\\n                                                        , col(\"energy_count\")).otherwise(0))\\\n                                                  .over(window))\n\n#------\n# Max\n#------\n\ndf = df.withColumn(\"energy_max_3months\", max(when(df.day_date \\\n                                                             >= (date(2014,2,28) - relativedelta(months=3))\\\n                                                        , col(\"energy_max\")).otherwise(0))\\\n                                                  .over(window))\n\ndf = df.withColumn(\"energy_max_6months\", max(when(df.day_date \\\n                                                             >= (date(2014,2,28) - relativedelta(months=6))\\\n                                                        , col(\"energy_max\")).otherwise(0))\\\n                                                  .over(window))\n\ndf = df.withColumn(\"energy_max_1yr\", max(when(df.day_date \\\n                                                             >= (date(2014,2,28) - relativedelta(months=12))\\\n                                                        , col(\"energy_max\")).otherwise(0))\\\n                                                  .over(window))\n\n#------\n# Mean\n#------\n\ndf = df.withColumn(\"energy_mean_3months\", avg(when(df.day_date \\\n                                                             >= (date(2014,2,28) - relativedelta(months=3))\\\n                                                        , col(\"energy_mean\")).otherwise(0))\\\n                                                  .over(window))\n\ndf = df.withColumn(\"energy_mean_6months\", avg(when(df.day_date \\\n                                                             >= (date(2014,2,28) - relativedelta(months=6))\\\n                                                        , col(\"energy_mean\")).otherwise(0))\\\n                                                  .over(window))\n\ndf = df.withColumn(\"energy_mean_1yr\", avg(when(df.day_date \\\n                                                             >= (date(2014,2,28) - relativedelta(months=12))\\\n                                                        , col(\"energy_mean\")).otherwise(0))\\\n                                                  .over(window))\n\n\n#------\n# Stddev\n#------\n\ndf = df.withColumn(\"energy_stddev_3months\", stddev(when(df.day_date \\\n                                                             >= (date(2014,2,28) - relativedelta(months=3))\\\n                                                        , col(\"energy_sum\")).otherwise(0))\\\n                                                  .over(window))\n\ndf = df.withColumn(\"energy_stddev_6months\", stddev(when(df.day_date \\\n                                                             >= (date(2014,2,28) - relativedelta(months=6))\\\n                                                        , col(\"energy_sum\")).otherwise(0))\\\n                                                  .over(window))\n\ndf = df.withColumn(\"energy_stddev_1yr\", stddev(when(df.day_date \\\n                                                             >= (date(2014,2,28) - relativedelta(months=12))\\\n                                                        , col(\"energy_sum\")).otherwise(0))\\\n                                                  .over(window))\n\ndf.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "### Create table in online store\n The following command will create a table in Amazon Keyspaces that will hold our new features. You may see an error \"AnalysisException: Couldn't find 800KEYSPACE.energy_data_features or any similarly named keyspace and table pairs\". This is because Keypaces creates tables asynchronously.   ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "spark.sql(\"\"\"\nCREATE TABLE myCatalog.800KEYSPACE.energy_data_features (\n     id String,\n    day_date Date,\n    energy_median Double,\n    energy_mean Double,\n    energy_max Double,\n    energy_count Int,\n    energy_std Double,\n    energy_sum Double,\n    energy_min Double,\n    energy_sum_3months Double,\n    energy_sum_6months Double,\n    energy_sum_1yr Double,\n    energy_count_3months Long,\n    energy_count_6months Long,\n    energy_count_1yr Long,\n    energy_max_3months Double,\n    energy_max_6months Double,\n    energy_max_1yr Double,\n    energy_mean_3months Double,\n    energy_mean_6months Double,\n    energy_mean_1yr Double,\n    energy_stddev_3months Double,\n    energy_stddev_6months Double,\n    energy_stddev_1yr Double)\n  PARTITIONED BY (id)\n  TBLPROPERTIES (\n    clustering_key='day_date.asc'\n  )\n\"\"\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "### Pad Data\n\nThe following will replace null values with 0 for data quality. In Keyspaces it could be stored be stored by leaving the field unset. ",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "df = df.selectExpr('household_id as id','day_date','energy_median','energy_mean','energy_max','energy_count','energy_std',\\\n               'energy_sum','energy_min','energy_sum_3months','energy_sum_6months','energy_sum_1yr',\\\n               'energy_count_3months','energy_count_6months','energy_count_1yr','energy_max_3months',\\\n               'energy_max_6months','energy_max_1yr','energy_mean_3months','energy_mean_6months','energy_mean_1yr',\\\n               'energy_stddev_3months','energy_stddev_6months','energy_stddev_1yr').fillna(0)\n\nprint(\"Records in Feature Dataset: {0:,}\".format(df.count()))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Write the data in the DynamicFrame to the offline store Delta store.\n\nNow that we have developed several features and prepared the data, we then store the features in the offline store.  You will first create a new s3 bucket and use spark and delta lake to persist the features. \n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import boto3\n# Create a new bucket for the data lake\n\ndf.write.format(\"delta\")\\\n        .mode(\"overwrite\")\\\n        .partitionBy('day_date')\\\n        .save(\"s3://800S3BUCKET/offline-feature-store\")\n\nprint(\"Saved data to offline store\")",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Write the data in the DynamicFrame to the oneline store in Amazon Keyspaces.\n\nNow next will move the features to the online store in Amazon Keyspaces. Glue/Spark has at-least once garentees for data persistence. If the Job complete, writes to both online and offline have been delivered at-least once. It is also possible to run a second anti-entropy job to compare like for like features using spark extensions. ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df.write.format(\"org.apache.spark.sql.cassandra\").mode(\"append\").option(\"keyspace\", '800KEYSPACE').option(\"table\", 'energy_data_features').save()\n\nprint(\"Saved data to online store\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "### Simulate random reads\n\nTo test our online store read performance we run a mix of random reads based on the id look ups. After the job runs you can check latencies in CloudWatch.  ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import rand \n\ntableDf = spark.read.format(\"org.apache.spark.sql.cassandra\").option(\"keyspace\", '800KEYSPACE').option(\"table\", 'energy_data_features').load() \n\ncond = [tableDf.id == df.id, tableDf.day_date == df.day_date]\n\ntotalreads = df.orderBy(rand()).join(tableDf, cond, \"left\").count()\n\nprint(\"Records in Feature Dataset: {0:,}\".format(totalreads))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "### Conclusion\n\nAWS Glue, Amazon S3, and Amazon Keyspaces provide a flexible and scalable way to store, share, and manage ML model features for training and inference to promote feature reuse across ML applications. Glue, S3, and Keyspaces are serverless data services. Developers can build in isolation with full capabilities. Teams can scale production services with virtually unbounded capacity. Ingest features from any data source including streaming and batch such as application logs, service logs, clickstreams, sensors, and tabular data from AWS or third party data sources. Transform data into ML features and build feature pipelines using Glue that support MLOps practices and speed time to model deployment. \n\n\n",
			"metadata": {}
		}
	]
}